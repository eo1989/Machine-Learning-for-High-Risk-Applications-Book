{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b42d651",
   "metadata": {},
   "source": [
    "<a href=\"https://githubtocolab.com/ml-for-high-risk-apps-book/Machine-Learning-for-High-Risk-Applications-Book/blob/main/code/Chapter-7/3.Transfer%20learning-Stage_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbe0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay, classification_report\n",
    "from skimage.util import random_noise\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from PIL import Image\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Seed\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "random_seed = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73f9c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff4246a1df0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241236e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422, 977)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_train_ds = ImageFolder('chest_xray_pre-processed/chest_xray/Cropped', \n",
    "                        transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                            transforms.RandomRotation(10),\n",
    "                                            transforms.RandomGrayscale(),\n",
    "                                            transforms.RandomAffine(translate=(0.05,0.05), degrees=0),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                           ]))\n",
    "val_ds = ImageFolder('chest_xray_pre-processed/chest_xray/val', \n",
    "                        transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                           ]))\n",
    "\n",
    "len(cropped_train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db8d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "\n",
    "cropped_train_dl = DataLoader(cropped_train_ds, batch_size, shuffle=True, worker_init_fn=seed_worker)\n",
    "val_dl = DataLoader(val_ds, batch_size, worker_init_fn=seed_worker)\n",
    "loaders = {'train':cropped_train_dl, 'val':val_dl}\n",
    "dataset_sizes = {'train':len(cropped_train_ds), 'val':len(val_ds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b499208",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('Finetuning_Stage1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0736ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4c535cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd9985fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'train':[], 'val':[]}\n",
    "accuracies = {'train':[], 'val':[]}\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, scheduler, epochs):\n",
    "  since = time.time()\n",
    "  best_model = copy.deepcopy(model.state_dict())\n",
    "  best_acc = 0.0\n",
    "  for epoch in range(epochs):\n",
    "    for phase in ['train', 'val']:\n",
    "      if phase == 'train':\n",
    "        model.train()\n",
    "      else:\n",
    "        model.eval()\n",
    "      \n",
    "      running_loss = 0.0\n",
    "      running_corrects = 0.0\n",
    "\n",
    "      for inputs, labels in loaders[phase]:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(phase=='train'):\n",
    "          outp = model(inputs)\n",
    "          _, pred = torch.max(outp, 1)\n",
    "          loss = criterion(outp, labels)\n",
    "          loss.requires_grad = True\n",
    "        \n",
    "          if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()*inputs.size(0)\n",
    "        running_corrects += torch.sum(pred == labels.data)\n",
    "\n",
    "\n",
    "      epoch_loss = running_loss / dataset_sizes[phase]\n",
    "      epoch_acc = running_corrects.double()/dataset_sizes[phase]\n",
    "      losses[phase].append(epoch_loss)\n",
    "      accuracies[phase].append(epoch_acc)\n",
    "      if phase == 'train':\n",
    "        print('Epoch: {}/{}'.format(epoch+1, epochs))\n",
    "      print('{} - loss:{}, accuracy{}'.format(phase, epoch_loss, epoch_acc))\n",
    "    \n",
    "            \n",
    "      if phase == 'val' and epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "    scheduler.step()  \n",
    "  print('Best accuracy {}'.format(best_acc))\n",
    "\n",
    "  model.load_state_dict(best_model)\n",
    "  return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8cf8954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/17\n",
      "train - loss:0.16213886227604374, accuracy0.9360189573459716\n",
      "val - loss:0.05905295006321144, accuracy0.9815762538382805\n",
      "Epoch: 2/17\n",
      "train - loss:0.2181292337970146, accuracy0.9218009478672986\n",
      "val - loss:0.09303262661174307, accuracy0.9733879222108496\n",
      "Epoch: 3/17\n",
      "train - loss:0.17090152465336694, accuracy0.9312796208530807\n",
      "val - loss:0.09203716716300457, accuracy0.9723643807574207\n",
      "Epoch: 4/17\n",
      "train - loss:0.17591952452197743, accuracy0.9312796208530807\n",
      "val - loss:0.08194925019857345, accuracy0.9744114636642784\n",
      "Epoch: 5/17\n",
      "train - loss:0.1736818109572781, accuracy0.9241706161137442\n",
      "val - loss:0.07081017136284617, accuracy0.9785056294779939\n",
      "Epoch: 6/17\n",
      "train - loss:0.1652766755689377, accuracy0.9265402843601896\n",
      "val - loss:0.06797165959056645, accuracy0.9795291709314228\n",
      "Epoch: 7/17\n",
      "train - loss:0.17079675276498907, accuracy0.9454976303317536\n",
      "val - loss:0.08522375099469713, accuracy0.9744114636642784\n",
      "Epoch: 8/17\n",
      "train - loss:0.16720860655159112, accuracy0.9454976303317536\n",
      "val - loss:0.08462242321445095, accuracy0.9744114636642784\n",
      "Epoch: 9/17\n",
      "train - loss:0.1643253081076518, accuracy0.9360189573459716\n",
      "val - loss:0.0878552215935862, accuracy0.9733879222108496\n",
      "Epoch: 10/17\n",
      "train - loss:0.18233245928988073, accuracy0.9265402843601896\n",
      "val - loss:0.08086182687440344, accuracy0.9744114636642784\n",
      "Epoch: 11/17\n",
      "train - loss:0.18567422757993376, accuracy0.9241706161137442\n",
      "val - loss:0.06240757423052341, accuracy0.9815762538382805\n",
      "Epoch: 12/17\n",
      "train - loss:0.17861574831732077, accuracy0.9336492890995262\n",
      "val - loss:0.06742130681486361, accuracy0.9805527123848516\n",
      "Epoch: 13/17\n",
      "train - loss:0.1907900203354833, accuracy0.9336492890995262\n",
      "val - loss:0.0762084919125589, accuracy0.9744114636642784\n",
      "Epoch: 14/17\n",
      "train - loss:0.162779003488765, accuracy0.9407582938388627\n",
      "val - loss:0.08570337082003851, accuracy0.9733879222108496\n",
      "Epoch: 15/17\n",
      "train - loss:0.18446631447999115, accuracy0.9336492890995262\n",
      "val - loss:0.08623684697826443, accuracy0.9744114636642784\n",
      "Epoch: 16/17\n",
      "train - loss:0.1861739684496587, accuracy0.9383886255924171\n",
      "val - loss:0.07517468557685293, accuracy0.9754350051177073\n",
      "Epoch: 17/17\n",
      "train - loss:0.1731398391638887, accuracy0.9431279620853081\n",
      "val - loss:0.0682922940559919, accuracy0.9805527123848516\n",
      "Best accuracy 0.9815762538382805\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "epochs = 17\n",
    "model = train(model, criterion, optimizer, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f022b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Finetuning_Stage2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98bfee",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e654c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = ImageFolder('chest_xray_pre-processed/chest_xray/test', \n",
    "                           transform=torchvision.transforms.Compose([torchvision.transforms.Resize((224,224)), \n",
    "                                                 torchvision.transforms.ToTensor(),\n",
    "                                                 torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),                                                               \n",
    "                                                 \n",
    "                                                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c64639c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(testset, batch_size=256)\n",
    "model.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "beaf99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1) \n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds)), preds\n",
    "\n",
    "def validation(batch):\n",
    "        images,labels = batch\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        output = model(images)                                      \n",
    "        loss = F.cross_entropy(output, labels)                    \n",
    "        acc,predictions = accuracy(output, labels)                       \n",
    "        \n",
    "        return {'valid_loss': loss.detach(), 'valid_accuracy':acc.detach(), 'predictions':predictions.detach(), 'labels':labels.detach()}\n",
    "    \n",
    "@torch.no_grad()\n",
    "def test_predict(model, test_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    outputs = [validation(batch) for batch in test_dataloader] \n",
    "    batch_losses = [x['valid_loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()           \n",
    "    batch_accs = [x['valid_accuracy'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()             \n",
    "  \n",
    "    batch_preds = [pred for x in outputs for pred in x['predictions'].tolist()] \n",
    "    batch_labels = [label for x in outputs for label in x['labels'].tolist()]  \n",
    "    \n",
    "    print('test_loss: {:.4f}, test_acc: {:.4f}'\n",
    "          .format(epoch_loss.item(), epoch_acc.item()))\n",
    "    \n",
    "    return batch_preds, batch_labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e4e2e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.2626, test_acc: 0.9334\n"
     ]
    }
   ],
   "source": [
    "preds,labels = test_predict(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4901a11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm  = confusion_matrix(labels, preds)\n",
    "plot_confusion_matrix(cm,figsize=(8,6),cmap=plt.cm.Greys)\n",
    "plt.xticks(range(2), ['Normal', 'Pneumonia'])\n",
    "plt.yticks(range(2), ['Normal', 'Pneumonia'])\n",
    "plt.xlabel('Predicted Label',fontsize=10)\n",
    "plt.ylabel('True Label',fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c3153ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90       234\n",
      "           1       0.92      0.97      0.94       390\n",
      "\n",
      "    accuracy                           0.93       624\n",
      "   macro avg       0.93      0.91      0.92       624\n",
      "weighted avg       0.93      0.93      0.93       624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f766e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
